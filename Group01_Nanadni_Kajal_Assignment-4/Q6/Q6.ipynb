{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"17lF4a4tP9HmcRAJH1SudCA5dKufN8Rot","authorship_tag":"ABX9TyMnXdnutdoBxBfjmZcEJuw0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"viTv2F5KpdJ0","executionInfo":{"status":"ok","timestamp":1713954513753,"user_tz":-330,"elapsed":26911,"user":{"displayName":"RESEARCH WORK","userId":"05775428025462541456"}},"outputId":"26d22255-994e-4ead-9434-d52951f55505"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import networkx as nx\n","from keras.datasets import mnist\n","\n","def extract_sift_features(image):\n","    \"\"\"Extract SIFT features from an image.\"\"\"\n","    sift = cv2.SIFT_create()\n","    keypoints, descriptors = sift.detectAndCompute(image, None)\n","    return keypoints, descriptors\n","\n","def calculate_similarity(descriptor1, descriptor2, method='euclidean'):\n","    \"\"\"Calculate similarity/distance between two SIFT descriptors.\"\"\"\n","    if method == 'euclidean':\n","        return np.linalg.norm(descriptor1 - descriptor2)\n","    elif method == 'cosine':\n","        return np.dot(descriptor1, descriptor2) / (np.linalg.norm(descriptor1) * np.linalg.norm(descriptor2))\n","    # Add more similarity/distance methods as needed\n","\n","def construct_graph(images, labels, similarity_method='euclidean', threshold=100):\n","    \"\"\"Construct a graph using SIFT features and specified similarity method.\"\"\"\n","    G = nx.Graph()\n","    for i in range(len(images)):\n","        image = images[i]\n","        label = labels[i]  # MNIST labels are single integers\n","        keypoints, descriptors = extract_sift_features(image)\n","        if descriptors is not None and len(descriptors) > 0:  # Check if descriptors are available\n","            concatenated_descriptors = np.concatenate(descriptors, axis=0)\n","            descriptor_length = concatenated_descriptors.shape[0]\n","            G.add_node((i, label))\n","            for j in range(len(images)):\n","                if i != j:\n","                    other_image = images[j]\n","                    other_label = labels[j]  # MNIST labels are single integers\n","                    other_keypoints, other_descriptors = extract_sift_features(other_image)\n","                    if other_descriptors is not None and len(other_descriptors) > 0:  # Check if descriptors are available\n","                        other_concatenated_descriptors = np.concatenate(other_descriptors, axis=0)\n","                        other_descriptor_length = other_concatenated_descriptors.shape[0]\n","                        if descriptor_length == other_descriptor_length:\n","                            # Calculate similarity between descriptors\n","                            similarity = calculate_similarity(concatenated_descriptors, other_concatenated_descriptors, method=similarity_method)\n","                            # Add edge if similarity is above threshold\n","                            if similarity < threshold:\n","                                G.add_edge((i, label), (j, other_label), weight=similarity)\n","    return G\n","\n","\n","\n","\n","# Load MNIST dataset\n","(train_images, train_labels), (_, _) = mnist.load_data()\n","\n","# Choose a subset of images for demonstration\n","num_images = 100\n","subset_images = train_images[:num_images]\n","subset_labels = train_labels[:num_images]\n","\n","# Example usage:\n","graph = construct_graph(subset_images, subset_labels, similarity_method='cosine', threshold=0.9)\n","print(\"Number of nodes:\", graph.number_of_nodes())\n","print(\"Number of edges:\", graph.number_of_edges())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TfNDzPzepoUf","executionInfo":{"status":"ok","timestamp":1713955581367,"user_tz":-330,"elapsed":7815,"user":{"displayName":"RESEARCH WORK","userId":"05775428025462541456"}},"outputId":"056f3447-d78c-41aa-ec4e-75ccdeebc016"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of nodes: 98\n","Number of edges: 338\n"]}]},{"cell_type":"code","source":["pip install grakel"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AdrXHvM9upnV","executionInfo":{"status":"ok","timestamp":1713955854277,"user_tz":-330,"elapsed":8542,"user":{"displayName":"RESEARCH WORK","userId":"05775428025462541456"}},"outputId":"858f2c87-b6fb-4884-f722-98158e89365f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting grakel\n","  Downloading GraKeL-0.1.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from grakel) (1.25.2)\n","Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.10/dist-packages (from grakel) (3.0.10)\n","Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.10/dist-packages (from grakel) (1.2.2)\n","Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from grakel) (1.16.0)\n","Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from grakel) (0.18.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from grakel) (1.4.0)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19->grakel) (1.11.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19->grakel) (3.4.0)\n","Installing collected packages: grakel\n","Successfully installed grakel-0.1.10\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from grakel.kernels import WeisfeilerLehman\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load adjacency matrices from MUTAG_A.txt\n","adj_matrices = np.loadtxt(\"/content/drive/MyDrive/D22180_ACV/Group01_Nandani_Kajal_Assignment-3&4/Q6/MUTAG/MUTAG_A.txt\", delimiter=',', dtype=int)\n","\n","# Load graph indicator and graph labels\n","graph_indicator = np.loadtxt(\"/content/drive/MyDrive/D22180_ACV/Group01_Nandani_Kajal_Assignment-3&4/Q6/MUTAG/MUTAG_graph_indicator.txt\", dtype=int)\n","graph_labels = np.loadtxt(\"/content/drive/MyDrive/D22180_ACV/Group01_Nandani_Kajal_Assignment-3&4/Q6/MUTAG/MUTAG_graph_labels.txt\", dtype=int)\n","\n","# Create a list of graphs\n","graphs = []\n","labels = []\n","\n","# Split the adjacency matrices into individual graphs based on graph indicators\n","unique_graphs = np.unique(graph_indicator)\n","for graph_id in unique_graphs:\n","    graph_indices = np.where(graph_indicator == graph_id)[0]\n","    graph = adj_matrices[np.ix_(graph_indices, graph_indices)]  # Extract submatrix for the current graph\n","    graphs.append(graph)\n","    labels.append(graph_labels[graph_id - 1])  # Subtract 1 to account for 0-based indexing\n","\n","# Convert labels to numpy array\n","labels = np.array(labels)\n","\n","# Split dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(graphs, labels, test_size=0.2, random_state=42)\n","\n","# Create and initialize the WL kernel\n","wl_kernel = WeisfeilerLehman(n_iter=5, normalize=True)\n","\n","# Compute kernel matrices for training and testing data\n","K_train = wl_kernel.fit_transform(X_train)\n","K_test = wl_kernel.transform(X_test)\n","\n","# Train SVM classifier\n","clf = SVC(kernel='precomputed')\n","clf.fit(K_train, y_train)\n","\n","# Evaluate classifier\n","y_pred = clf.predict(K_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"HCerC5Gw6g0w","executionInfo":{"status":"error","timestamp":1713959054876,"user_tz":-330,"elapsed":10,"user":{"displayName":"RESEARCH WORK","userId":"05775428025462541456"}},"outputId":"8b0b81c1-958d-4bf5-d60f-43a2ae5548da"},"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"index 2 is out of bounds for axis 1 with size 2","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-180b23eaf674>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgraph_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_graphs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mgraph_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_indicator\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgraph_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj_matrices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Extract submatrix for the current graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph_id\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Subtract 1 to account for 0-based indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/D22180_ACV/Group01_Nandani_Kajal_Assignment-3&4/Q6/grakel"],"metadata":{"id":"fbxZg-BuJo5y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://storage.googleapis.com/download.tensorflow.org/data/mutag.zip\n","!unzip mutag.zip"],"metadata":{"id":"itTbql2bJl9v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install -q tensorflow-gnn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jUb9-6uiF8LF","executionInfo":{"status":"ok","timestamp":1713961991500,"user_tz":-330,"elapsed":44704,"user":{"displayName":"RESEARCH WORK","userId":"05775428025462541456"}},"outputId":"7b395497-c504-4cd2-af16-6fb5b6ba6eb1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m838.4/838.4 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m676.9/676.9 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import os\n","os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"  # For TF2.16+.\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_gnn as tfgnn\n","\n","print(f'Running TF-GNN {tfgnn.__version__} with TensorFlow {tf.__version__}.')\n","\n","\"\"\"### Download the MUTAG dataset\n","We have created a version of the MUTAG Dataset in TF-GNN's file format to use as an example in this colab.\n","\n","Citation: [Morris, Christopher, et al. Tudataset: A collection of benchmark datasets for learning with graphs. arXiv preprint arXiv:2007.08663. 2020.](https://chrsmrrs.github.io/datasets/)\n","\"\"\"\n","\n","# Download and unzip dataset.\n","#!wget https://storage.googleapis.com/download.tensorflow.org/data/mutag.zip\n","#!unzip mutag.zip\n","\n","train_path = os.path.join(os.getcwd(), 'mutag', 'train.tfrecords')\n","val_path = os.path.join(os.getcwd(), 'mutag', 'val.tfrecords')\n","!ls -l {train_path} {val_path}\n","\n","graph_tensor_spec = tfgnn.GraphTensorSpec.from_piece_specs(\n","    context_spec=tfgnn.ContextSpec.from_field_specs(features_spec={\n","                  'label': tf.TensorSpec(shape=(1,), dtype=tf.int32)\n","    }),\n","    node_sets_spec={\n","        'atoms':\n","            tfgnn.NodeSetSpec.from_field_specs(\n","                features_spec={\n","                    tfgnn.HIDDEN_STATE:\n","                        tf.TensorSpec((None, 7), tf.float32)\n","                },\n","                sizes_spec=tf.TensorSpec((1,), tf.int32))\n","    },\n","    edge_sets_spec={\n","        'bonds':\n","            tfgnn.EdgeSetSpec.from_field_specs(\n","                features_spec={\n","                    tfgnn.HIDDEN_STATE:\n","                        tf.TensorSpec((None, 4), tf.float32)\n","                },\n","                sizes_spec=tf.TensorSpec((1,), tf.int32),\n","                adjacency_spec=tfgnn.AdjacencySpec.from_incident_node_sets(\n","                    'atoms', 'atoms'))\n","    })\n","\n","\n","def decode_fn(record_bytes):\n","  graph = tfgnn.parse_single_example(\n","      graph_tensor_spec, record_bytes, validate=True)\n","\n","  # extract label from context and remove from input graph\n","  context_features = graph.context.get_features_dict()\n","  label = context_features.pop('label')\n","  new_graph = graph.replace_features(context=context_features)\n","\n","  return new_graph, label\n","\n","train_ds = tf.data.TFRecordDataset([train_path]).map(decode_fn)\n","val_ds = tf.data.TFRecordDataset([val_path]).map(decode_fn)\n","\n","\"\"\"### Look at one example from the dataset\"\"\"\n","\n","g, y = train_ds.take(1).get_single_element()\n","\n","\"\"\"#### Node features\n","\n","Node features represent the 1-hot encoding of the atom type (0=C, 1=N, 2=O, 3=F,\n","4=I, 5=Cl, 6=Br).\n","\"\"\"\n","\n","print(g.node_sets['atoms'].features[tfgnn.HIDDEN_STATE])\n","\n","\"\"\"#### Bond Edges\n","\n","In this example, we consider the bonds between atoms undirected edges. To encode\n","them in the GraphsTuple, we store the undirected edges as pairs of directed\n","edges in both directions.\n","\n","`adjacency.source` contains the source node indices, and `adjacency.target` contains the corresponding target node indices.\n","\"\"\"\n","\n","g.edge_sets['bonds'].adjacency.source\n","\n","g.edge_sets['bonds'].adjacency.target\n","\n","\"\"\"#### Edge features\n","\n","Edge features represent the bond type as one-hot encoding.\n","\"\"\"\n","\n","g.edge_sets['bonds'].features[tfgnn.HIDDEN_STATE]\n","\n","\"\"\"### Label\n","The label is binary, indicating the mutagenicity of the molecule. It's either 0 or 1.\n","\"\"\"\n","\n","y\n","\n","\"\"\"#### Batch the datasets\"\"\"\n","\n","batch_size = 32\n","train_ds_batched = train_ds.batch(batch_size=batch_size).repeat()\n","val_ds_batched = val_ds.batch(batch_size=batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZU4dxbGqFQFS","executionInfo":{"status":"ok","timestamp":1713961998540,"user_tz":-330,"elapsed":2939,"user":{"displayName":"RESEARCH WORK","userId":"05775428025462541456"}},"outputId":"50aed457-ec91-4228-8a8d-43a4edb32855"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running TF-GNN 1.0.2 with TensorFlow 2.15.0.\n","/bin/bash: line 1: 4/Q1/mutag/val.tfrecords: No such file or directory\n","/bin/bash: line 1: 4/Q1/mutag/train.tfrecords: No such file or directory\n","ls: cannot access '/content/drive/MyDrive/D22180_ACV/Group01_Nandani_Kajal_Assignment-3': No such file or directory\n","tf.Tensor(\n","[[1. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1.]\n"," [0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0.]], shape=(14, 7), dtype=float32)\n"]}]},{"cell_type":"code","source":["import os\n","os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"  # For TF2.16+.\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_gnn as tfgnn\n","import grakel\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","print(f'Running TF-GNN {tfgnn.__version__} with TensorFlow {tf.__version__}.')\n","\n","\"\"\"### Download the MUTAG dataset\n","We have created a version of the MUTAG Dataset in TF-GNN's file format to use as an example in this colab.\n","\n","Citation: [Morris, Christopher, et al. Tudataset: A collection of benchmark datasets for learning with graphs. arXiv preprint arXiv:2007.08663. 2020.](https://chrsmrrs.github.io/datasets/)\n","\"\"\"\n","\n","# Download and unzip dataset.\n","#!wget https://storage.googleapis.com/download.tensorflow.org/data/mutag.zip\n","#!unzip mutag.zip\n","\n","train_path = os.path.join(os.getcwd(), 'mutag', 'train.tfrecords')\n","val_path = os.path.join(os.getcwd(), 'mutag', 'val.tfrecords')\n","!ls -l {train_path} {val_path}\n","\n","graph_tensor_spec = tfgnn.GraphTensorSpec.from_piece_specs(\n","    context_spec=tfgnn.ContextSpec.from_field_specs(features_spec={\n","                  'label': tf.TensorSpec(shape=(1,), dtype=tf.int32)\n","    }),\n","    node_sets_spec={\n","        'atoms':\n","            tfgnn.NodeSetSpec.from_field_specs(\n","                features_spec={\n","                    tfgnn.HIDDEN_STATE:\n","                        tf.TensorSpec((None, 7), tf.float32)\n","                },\n","                sizes_spec=tf.TensorSpec((1,), tf.int32))\n","    },\n","    edge_sets_spec={\n","        'bonds':\n","            tfgnn.EdgeSetSpec.from_field_specs(\n","                features_spec={\n","                    tfgnn.HIDDEN_STATE:\n","                        tf.TensorSpec((None, 4), tf.float32)\n","                },\n","                sizes_spec=tf.TensorSpec((1,), tf.int32),\n","                adjacency_spec=tfgnn.AdjacencySpec.from_incident_node_sets(\n","                    'atoms', 'atoms'))\n","    })\n","\n","\n","def decode_fn(record_bytes):\n","  graph = tfgnn.parse_single_example(\n","      graph_tensor_spec, record_bytes, validate=True)\n","\n","  # extract label from context and remove from input graph\n","  context_features = graph.context.get_features_dict()\n","  label = context_features.pop('label')\n","  new_graph = graph.replace_features(context=context_features)\n","\n","  return new_graph, label\n","\n","train_ds = tf.data.TFRecordDataset([train_path]).map(decode_fn)\n","val_ds = tf.data.TFRecordDataset([val_path]).map(decode_fn)\n","\n","\"\"\"### Look at one example from the dataset\"\"\"\n","\n","g, y = train_ds.take(1).get_single_element()\n","\n","\"\"\"#### Node features\n","\n","Node features represent the 1-hot encoding of the atom type (0=C, 1=N, 2=O, 3=F,\n","4=I, 5=Cl, 6=Br).\n","\"\"\"\n","\n","print(g.node_sets['atoms'].features[tfgnn.HIDDEN_STATE])\n","\n","\"\"\"#### Bond Edges\n","\n","In this example, we consider the bonds between atoms undirected edges. To encode\n","them in the GraphsTuple, we store the undirected edges as pairs of directed\n","edges in both directions.\n","\n","`adjacency.source` contains the source node indices, and `adjacency.target` contains the corresponding target node indices.\n","\"\"\"\n","\n","g.edge_sets['bonds'].adjacency.source\n","\n","g.edge_sets['bonds'].adjacency.target\n","\n","\"\"\"#### Edge features\n","\n","Edge features represent the bond type as one-hot encoding.\n","\"\"\"\n","\n","g.edge_sets['bonds'].features[tfgnn.HIDDEN_STATE]\n","\n","\"\"\"### Label\n","The label is binary, indicating the mutagenicity of the molecule. It's either 0 or 1.\n","\"\"\"\n","\n","y\n","\n","\"\"\"#### Batch the datasets\"\"\"\n","\n","batch_size = 32\n","train_ds_batched = train_ds.batch(batch_size=batch_size).repeat()\n","val_ds_batched = val_ds.batch(batch_size=batch_size)\n","\n","# Convert TensorFlow datasets to GraKeL-compatible format\n","X_train, y_train = [], []\n","for graph, label in train_ds_batched:\n","    # Convert graph to GraKeL-compatible format\n","    # Append to X_train and y_train\n","    X_train.append(graph)\n","    y_train.append(label)\n","\n","X_val, y_val = [], []\n","for graph, label in val_ds_batched:\n","    # Convert graph to GraKeL-compatible format\n","    # Append to X_val and y_val\n","    X_val.append(graph)\n","    y_val.append(label)\n","\n","# Step 3: Experiment with Graph Kernels\n","# Example: Use WL kernel\n","wl_kernel = grakel.WeisfeilerLehman(n_iter=5, normalize=True)\n","K_train = wl_kernel.fit_transform(X_train)\n","K_val = wl_kernel.transform(X_val)\n","\n","# Step 4: Train and Evaluate Models\n","svm_classifier = SVC(kernel='precomputed')\n","svm_classifier.fit(K_train, y_train)\n","predictions = svm_classifier.predict(K_val)\n","\n","# Step 5: Compare Performance\n","accuracy = accuracy_score(y_val, predictions)\n","print(\"Accuracy:\", accuracy)\n","\n","# Step 6: Present Results\n","# Visualize and present the results as needed\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xzE-BkUxF2Rj","outputId":"d4fa3073-d5de-43b3-c2e4-c1d7b57c9bc2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running TF-GNN 1.0.2 with TensorFlow 2.15.0.\n","/bin/bash: line 1: 4/Q1/mutag/val.tfrecords: No such file or directory\n","/bin/bash: line 1: 4/Q1/mutag/train.tfrecords: No such file or directory\n","ls: cannot access '/content/drive/MyDrive/D22180_ACV/Group01_Nandani_Kajal_Assignment-3': No such file or directory\n","tf.Tensor(\n","[[1. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1.]\n"," [0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0.]], shape=(14, 7), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["#WL kernel"],"metadata":{"id":"VQH28gO_rSXD"}},{"cell_type":"code","source":["import numpy as np\n","import networkx as nx\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from keras.datasets import mnist\n","\n","# Load MNIST dataset\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","\n","# Normalize pixel values\n","X_train = X_train.astype('float32') / 255.0\n","X_test = X_test.astype('float32') / 255.0\n","\n","# Reshape images to (28, 28)\n","X_train = X_train.reshape(-1, 28, 28)\n","X_test = X_test.reshape(-1, 28, 28)\n","\n","# Encode labels\n","label_encoder = LabelEncoder()\n","y_train_encoded = label_encoder.fit_transform(y_train)\n","y_test_encoded = label_encoder.transform(y_test)\n","\n","# Define function to convert image to graph\n","def image_to_graph(image):\n","    # Convert image to binary\n","    threshold = 0.5\n","    binary_image = (image > threshold).astype(int)\n","\n","    # Create graph\n","    G = nx.grid_2d_graph(28, 28)\n","\n","    # Add nodes and edges\n","    for i in range(28):\n","        for j in range(28):\n","            if binary_image[i, j] == 1:\n","                G.nodes[(i, j)]['label'] = 1  # Pixel is white\n","                if i > 0 and binary_image[i-1, j] == 1:\n","                    G.add_edge((i, j), (i-1, j))\n","                if i < 27 and binary_image[i+1, j] == 1:\n","                    G.add_edge((i, j), (i+1, j))\n","                if j > 0 and binary_image[i, j-1] == 1:\n","                    G.add_edge((i, j), (i, j-1))\n","                if j < 27 and binary_image[i, j+1] == 1:\n","                    G.add_edge((i, j), (i, j+1))\n","\n","    return G\n","\n","# Convert images to graphs\n","X_train_graphs = [image_to_graph(image) for image in X_train[:1000]]  # Taking subset for demonstration\n","X_test_graphs = [image_to_graph(image) for image in X_test[:1000]]    # Taking subset for demonstration\n"],"metadata":{"id":"zr3yvPuJrHXD","executionInfo":{"status":"ok","timestamp":1715163038645,"user_tz":-330,"elapsed":14708,"user":{"displayName":"RESEARCH WORK","userId":"05775428025462541456"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Compute WL kernel matrix\n","kernel_matrix_train = wl_kernel(X_train_graphs, iterations=3)\n","kernel_matrix_test = wl_kernel(X_test_graphs, iterations=3)"],"metadata":{"id":"pxHhRtLyqs85","executionInfo":{"status":"ok","timestamp":1715163338551,"user_tz":-330,"elapsed":296924,"user":{"displayName":"RESEARCH WORK","userId":"05775428025462541456"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from sklearn import svm\n","from sklearn.metrics import accuracy_score\n","\n","# Train SVM classifier\n","clf = svm.SVC(kernel='precomputed')\n","clf.fit(kernel_matrix_train, y_train_encoded[:1000])  # Using subset of training labels for demonstration\n","\n","# Predict labels for test data\n","y_pred = clf.predict(kernel_matrix_test)\n","\n","# Evaluate accuracy\n","accuracy = accuracy_score(y_test_encoded[:1000], y_pred)  # Using subset of test labels for demonstration\n","print(\"Accuracy:\", accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hd67rp3CqDi3","executionInfo":{"status":"ok","timestamp":1715163761049,"user_tz":-330,"elapsed":411,"user":{"displayName":"RESEARCH WORK","userId":"05775428025462541456"}},"outputId":"2b8195ee-500c-4f92-9be1-b5797870e34b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.099\n"]}]},{"cell_type":"markdown","source":["WL-Kernel"],"metadata":{"id":"PwULd3pFsRbK"}},{"cell_type":"code","source":["# Define WL kernel function\n","def wl_kernel(graphs, iterations):\n","    # Initialize kernel matrix\n","    n = len(graphs)\n","    kernel_matrix = np.zeros((n, n))\n","\n","    for i, G1 in enumerate(graphs):\n","        # Initialize node labels\n","        node_labels = {node: 0 for node in G1.nodes()}\n","\n","        for _ in range(iterations):\n","            # Update node labels based on neighbor labels\n","            new_labels = {}\n","            for node in G1.nodes():\n","                neighbors = G1.neighbors(node)\n","                neighbor_labels = tuple(sorted(node_labels[n] for n in neighbors))\n","                new_labels[node] = neighbor_labels\n","\n","            # Update node labels\n","            node_labels = new_labels\n","\n","        # Compute kernel values\n","        for j, G2 in enumerate(graphs):\n","            kernel_matrix[i, j] = len([node for node in G1.nodes() if node_labels[node] == node_labels[node]])\n","\n","    return kernel_matrix\n","\n","# Compute WL kernel matrix\n","kernel_matrix_train = wl_kernel(X_train_graphs, iterations=10)\n","kernel_matrix_test = wl_kernel(X_test_graphs, iterations=10)\n"],"metadata":{"id":"kWJ09P94rNzq","executionInfo":{"status":"ok","timestamp":1715166739487,"user_tz":-330,"elapsed":2946924,"user":{"displayName":"RESEARCH WORK","userId":"05775428025462541456"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from sklearn import svm\n","from sklearn.metrics import accuracy_score\n","\n","# Train SVM classifier\n","clf = svm.SVC(kernel='precomputed')\n","clf.fit(kernel_matrix_train, y_train_encoded[:1000])  # Using subset of training labels for demonstration\n","\n","# Predict labels for test data\n","y_pred = clf.predict(kernel_matrix_test)\n","\n","# Evaluate accuracy\n","accuracy = accuracy_score(y_test_encoded[:1000], y_pred)  # Using subset of test labels for demonstration\n","print(\"Accuracy:\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fcVec43Ns9ML","executionInfo":{"status":"ok","timestamp":1715163767379,"user_tz":-330,"elapsed":423,"user":{"displayName":"RESEARCH WORK","userId":"05775428025462541456"}},"outputId":"98b71d08-6dbf-458c-f054-a9d2fe480fef"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.099\n"]}]},{"cell_type":"markdown","source":["#MUTAG"],"metadata":{"id":"fKOZ8YHTvxAP"}},{"cell_type":"code","source":["pip install grakel\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YVSXF5R7v2Yw","executionInfo":{"status":"ok","timestamp":1715166751467,"user_tz":-330,"elapsed":10273,"user":{"displayName":"RESEARCH WORK","userId":"05775428025462541456"}},"outputId":"26e87099-5a35-4569-ac78-d1f0868d1163"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting grakel\n","  Downloading GraKeL-0.1.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from grakel) (1.25.2)\n","Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.10/dist-packages (from grakel) (3.0.10)\n","Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.10/dist-packages (from grakel) (1.2.2)\n","Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from grakel) (1.16.0)\n","Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from grakel) (0.18.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from grakel) (1.4.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19->grakel) (1.11.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19->grakel) (3.5.0)\n","Installing collected packages: grakel\n","Successfully installed grakel-0.1.10\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from scipy.io import loadmat\n","from grakel.datasets import fetch_dataset\n","from grakel.kernels import ShortestPath\n","\n","# Load MUTAG dataset\n","mutag_dataset = loadmat('mutag.mat')\n","graphs = mutag_dataset['MUTAG'][0]\n","\n","# Convert dataset to a format compatible with GraKeL\n","# Assuming graphs is a list of NetworkX graphs\n","\n","# Define graph kernel\n","kernel = ShortestPath()\n","\n","# Compute graph kernel matrix\n","kernel_matrix = kernel.fit_transform(graphs)\n","\n","# Now you can use kernel_matrix as input for machine learning models\n"],"metadata":{"id":"MAM2Sjpxvvig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PAH4MboLuhyO"},"execution_count":null,"outputs":[]}]}